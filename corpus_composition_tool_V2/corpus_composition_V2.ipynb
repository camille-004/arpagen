{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d29ca21-48fb-469e-ac71-bd68ba108694",
   "metadata": {},
   "source": [
    "#### Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ae1dbf2-c1b9-46e7-9647-a84dd34e65f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import re\n",
    "from string import punctuation\n",
    "from time import time\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6476c1fb-df27-478a-bf9c-f7f273cf77e9",
   "metadata": {},
   "source": [
    "#### Arpabet Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "287d326a-bbe3-408e-a725-a41030d01b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/33666557/get-phonemes-from-any-word-in-python-nltk-or-other-modules\n",
    "#Set arpabet to nltk cmudict corpus. If cmudict unavalible, download it then set.\n",
    "try:\n",
    "    arpabet = nltk.corpus.cmudict.dict()\n",
    "except LookupError:\n",
    "    nltk.download('cmudict')\n",
    "    arpabet = nltk.corpus.cmudict.dict()\n",
    "\n",
    "#Keeping only the first set of phonemes\n",
    "for phoneme in arpabet: arpabet[phoneme] = arpabet[phoneme][0]\n",
    "\n",
    "accepted_words = list(arpabet.keys())\n",
    "accepted_phonemes = list(arpabet.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8c3aaa-cf49-41bd-b552-9630262362f2",
   "metadata": {},
   "source": [
    "#### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afa70d03-7e79-4efd-9a47-69de13ea3913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define translation from word to phonetics\n",
    "# If word not in cmudict dictionary, find closest match (Jaccard Distance)\n",
    "def word_to_phonetic(word):\n",
    "    try:\n",
    "        phonetic = arpabet[word]\n",
    "    except:\n",
    "        closest = [(jaccard_distance(set(ngrams(word, 2)),set(ngrams(w, 2))),w) for w in accepted_words if w[0]==word[0]]\n",
    "        phonetic = arpabet[min(closest, key = lambda d: d[0])[1]]\n",
    "    return phonetic\n",
    "\n",
    "# Define translation from phonetics to words\n",
    "# If word not in cmudict dictionary, find closest match (Jaccard Distance)\n",
    "def phonetic_to_word(phonemes):\n",
    "    try:\n",
    "        word = accepted_words[accepted_phonemes.index(phonemes)]\n",
    "    except:\n",
    "        closest = [(jaccard_distance(set(ngrams(phonemes, 2)),set(ngrams(w, 2))),w) for w in accepted_phonemes]\n",
    "        word = accepted_words[accepted_phonemes.index(min(closest, key = lambda t: t[0])[1])]\n",
    "    return word\n",
    "\n",
    "# Convert list of sentence to list of phoneme lists\n",
    "def sentences_to_phonemes(data):\n",
    "    return [sentence_to_phonemes(sentence) for sentence in tqdm(data)]\n",
    "\n",
    "def sentence_to_phonemes(sentence):\n",
    "    return [word_to_phonetic(word) for word in sentence.split(' ')]\n",
    "\n",
    "# Convert list of phoneme lists to sentences\n",
    "def phonemes_to_sentence(data):\n",
    "    return [phoneme_to_sentence(sentence) for sentence in tqdm(data)]\n",
    "\n",
    "def phoneme_to_sentence(phonemes):\n",
    "    return ' '.join([phonetic_to_word(phoneme) for phoneme in phonemes])\n",
    "\n",
    "# Pre-processing of *.txt into sentences\n",
    "def text_to_sentences(data, split_str='\\.|\\!|\\?', remove_chars=r'[^a-z\\' ]+'):\n",
    "    my_punctuation = punctuation.replace(\"'\", \"\")\n",
    "    data = re.split(split_str, data)\n",
    "    data = [d.replace('\\n', ' ') for d in data]\n",
    "    data = [re.sub(remove_chars, '', d.lower()).lstrip() for d in data]\n",
    "    data = [\" \".join(d.split()) for d in data if len(d) != 0]\n",
    "    return data\n",
    "\n",
    "# Output phonemes in json format\n",
    "def to_json(jsonOutput_name, data):\n",
    "    data_as_dict = {\"Sentence \"+str(i+1):{\"Word \"+str(l+1):{\"Phoneme \"+str(m+1):n for (m, n) in enumerate(k)} for (l, k) in enumerate(j)} for (i, j) in enumerate(data)}\n",
    "    jsonOutput = open(jsonOutput_name, \"w\")\n",
    "    jsonOutput.write(json.dumps(data_as_dict, indent=4))\n",
    "    jsonOutput.close()\n",
    "\n",
    "\n",
    "# Import phonemes from json format\n",
    "def from_json(jsonInput_file):\n",
    "    data = json.load(open(jsonInput_file))\n",
    "    data = [[list(n.values()) for n in list(m.values())] for m in list(data.values())]\n",
    "    return data\n",
    "\n",
    "def arpabet_recompiler(arpabet, data, export_file='arpabet.json'):\n",
    "    #Data preprocessing\n",
    "    data = data.replace('\\n', ' ')\n",
    "    data = re.sub(r'[^a-z\\' ]+', '', data.lower())\n",
    "    data = re.split(' ', data)\n",
    "    data = [\" \".join(d.split()) for d in data if len(d) != 0]\n",
    "    \n",
    "    #Removing all unused arpabet words\n",
    "    arpabet_set, data_set = set(arpabet), set(data)\n",
    "    for word in tqdm(data_set - arpabet_set):\n",
    "        closest = [(jaccard_distance(set(ngrams(word, 2)),set(ngrams(w, 2))),w) for w in accepted_words if w[0]==word[0]]\n",
    "        data_set.add(min(closest, key = lambda d: d[0])[1])\n",
    "    for k in arpabet_set - data_set:\n",
    "        del arpabet[k]\n",
    "        \n",
    "    #Export new arpabet dictionary\n",
    "    with open('arpabet.json', 'w') as outfile:\n",
    "        json.dump(arpabet, outfile)\n",
    "    \n",
    "    return arpabet\n",
    "\n",
    "def arpabet_reader(import_file):\n",
    "    with open('arpabet.json') as in_file:\n",
    "        arpabet = json.load(in_file)\n",
    "    return arpabet, list(arpabet.keys()), list(arpabet.values())\n",
    "\n",
    "def RNN_out_to_str(input_string):\n",
    "    sentences = input_string.replace('<BOS>', '').split('<EOS>')\n",
    "    sentences = [[phonetic_to_word(l.split()) for l in n.split('<SPACE>')] for n in sentences]\n",
    "    sentences = '. '.join([' '.join(i).capitalize() for i in sentences])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d77fe65-1827-44de-a832-cdbd198e7b5e",
   "metadata": {},
   "source": [
    "#### English/Phonetics Translation Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56cd5bf6-72a4-4178-b46e-71d886124c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Recompiling Arpabet -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b37a7d7e90244bcb84409b030e3580a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5379 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Transcribing sentences to phonemes -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4992e0c9b9440159542e35dda20fce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example phonetic transcription: [['IH0', 'N'], ['DH', 'AH0'], ['B', 'IH0', 'G', 'IH1', 'N', 'IH0', 'NG'], ['G', 'AA1', 'D'], ['K', 'R', 'IY0', 'EY1', 'T', 'AH0', 'D'], ['DH', 'AH0'], ['HH', 'EH1', 'V', 'AH0', 'N'], ['AH0', 'N', 'D'], ['DH', 'AH0'], ['ER1', 'TH']] \n",
      "\n",
      "----- Transcribing phonemes to sentences -----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bf8d0476414209b77d455063293371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example English transcription: in the beginning god created the heaven and the earth \n",
      "\n",
      "--- Total Time Elapsed: 207.59759044647217 seconds ---\n",
      "\n",
      "Export/import path matches original values:  True\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "\n",
    "print(\"----- Recompiling Arpabet -----\")\n",
    "file = open(\"bible.txt\")\n",
    "arpabet = arpabet = arpabet_recompiler(arpabet, file.read())\n",
    "file.close()\n",
    "print(\"\")\n",
    "\n",
    "file = open(\"bible.txt\")\n",
    "data = text_to_sentences(file.read())\n",
    "data = data[:500]\n",
    "arpabet, accepted_words, accepted_phonemes = arpabet_reader('arpabet.pkl')\n",
    "\n",
    "print(\"----- Transcribing sentences to phonemes -----\")\n",
    "p = sentences_to_phonemes(data)\n",
    "print(\"Example phonetic transcription:\", p[0], \"\\n\")\n",
    "\n",
    "print(\"----- Transcribing phonemes to sentences -----\")\n",
    "s = phonemes_to_sentence(p)\n",
    "print(\"Example English transcription:\", s[0],\"\\n\")\n",
    "\n",
    "# close file\n",
    "file.close()\n",
    "\n",
    "print(\"--- Total Time Elapsed: %s seconds ---\\n\" % (time() - start_time))\n",
    "\n",
    "to_json(\"example.json\", p)\n",
    "f = from_json('example.json')\n",
    "print(\"Export/import path matches original values: \", f == p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29b63e2d-bb14-4e94-aedd-6877017723c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"sentences_new.txt\")\n",
    "arpabet = nltk.corpus.cmudict.dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbaa959-7a3a-4f50-a027-1d3ccc2ed969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6f171d4-b4eb-4b15-a57d-00d0cb7725c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c753e21f3c46be9d9a42e939d97b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10144 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34800\\3368118250.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0marpabet_recompiler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marpabet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_34800\\41297983.py\u001b[0m in \u001b[0;36marpabet_recompiler\u001b[1;34m(arpabet, data, export_file)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_set\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0marpabet_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mclosest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjaccard_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maccepted_words\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdata_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclosest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marpabet_set\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0marpabet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "arpabet_recompiler(arpabet, file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b90c27-bac2-49d5-b498-849cb2a9372c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
